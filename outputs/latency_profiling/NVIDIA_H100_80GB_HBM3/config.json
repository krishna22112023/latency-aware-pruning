{
    "base_model": "meta-llama/Llama-3.2-1B",
    "lora_r": 8,
    "lora_alpha": 16,
    "lora_dropout": 0,
    "lora_target_modules": [
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj"
    ],
    "batch_size": 1,
    "device": "cuda",
    "device_name": "NVIDIA H100 80GB HBM3",
    "seq_lens_to_profile": [
        128,
        256,
        384,
        512,
        768,
        1024,
        2048
    ],
    "head_counts_to_profile": [
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9,
        10,
        11,
        12,
        13,
        14,
        15,
        16,
        17,
        18,
        19,
        20,
        21,
        22,
        23,
        24,
        25,
        26,
        27,
        28,
        29,
        30,
        31,
        32
    ],
    "ffn_counts_to_profile": [
        256,
        512,
        1024,
        1536,
        2048,
        2560,
        3072,
        3584,
        4096,
        4608,
        5120,
        5632,
        6144,
        6656,
        7168,
        7680,
        8192
    ]
}